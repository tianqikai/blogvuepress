
# 6. 分布式

## 1 雪花算法原理

第一位符号位固定为0，41位时间戳，10位workId，12位序列号，位数可以有不同实现
优点：
每个毫秒值包含的ID值很多，不够可以变动位数来增加，性能佳（依赖workId的实现）。
时间戳值在高位，中间是固定的机器码，自增的序列在低位，整个ID是趋势递增的。
能够根据业务场景数据库节点布置灵活调整bit位划分，灵活度高。
缺点：
强依赖于机器时钟，如果时钟回拨，会导致重复的ID生成，所以一般基于此的算法发现时钟回拨，都会抛异常处理，阻止ID生成，这可能导致服务不可用。



## 2 数据库实现分布式锁的问题及解决方案
利用唯一约束键存储key，insert成功则代表获取锁成功，失败则获取失败，操作完成需要删除锁
问题：

非阻塞，锁获取失败后没有排队机制，需要自己编码实现阻塞，可以`使用自旋，直到获取锁`

`不可重入`，如果加锁的方法需要递归，则第二次插入会失败，可以使用记录线程标识解决重入问题

`死锁`，删除锁失败、则其他线程没办法获取锁，`可以设置超时时间、使用定时任务检查`
`数据库单点故障，数据库高可用`

## 3 数据一致性模型有哪些
`强一致性`：当更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。根据 CAP 理论，这种实现需要牺牲可用性。
`弱一致性`：系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。
`最终一致性`：最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而
不需要实时保证系统数据的强一致性。到达最终一致性的时间 ，就是不一致窗口时间，在没有故障发生
的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。
最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括因果一致性和会话一致性等。


`因果一致性`：要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。
进程 A 在更新完某个数据项后通知了进程 B，那么进程 B 之后对该数据项的访问都应该能够获取到进程
A 更新后的最新值，并且如果进程 B 要对该数据项进行更新操作的话，务必基于进程 A 更新后的最新
值。
在微博或者微信进行评论的时候，比如你在朋友圈发了一张照片，朋友给你评论了，而你对朋友的评论
进行了回复，这条朋友圈的显示中，你的回复必须在朋友之后，这是一个因果关系，而其他没有因果关
系的数据，可以允许不一致。
`会话一致性`：将对系统数据的访问过程框定在了一个会话当中，约定了系统能保证在同一个有效的会话
中实现“读己之所写”的一致性，就是在你的一次访问中，执行更新操作之后，客户端能够在同一个会话
中始终读取到该数


## 4 什么是分布式事务？ 有哪些实现⽅案？

在分布式系统中， ⼀次业务处理可能需要多个应⽤来实现， ⽐如⽤户发送⼀次下单请求， 就涉及到订单系统创建订单 、库存系统减库存， ⽽对于⼀次下单， 订单创建与减库存应该是要同时成功或同时失败的， 但在分布式系统中， 如果不做处理， 就很有可能出现订单创建成功， 但是减库存失败， 那么解决这 类问题， 就需要⽤到分布式事务 。

:::tip 常⽤解决⽅案有：
1. **本地消息表**： 创建订单时， 将减库存消息加⼊在本地事务中， ⼀起提交到数据库存⼊本地消息表，
然后调⽤库存系统， 如果调⽤成功则修改本地消息状态为成功， 如果调⽤库存系统失败， 则由后台定时任务从本地消息表中取出未成功的消息， 重试调⽤库存系统
2. **消息队列**：  ⽬前RocketMQ中⽀持事务消息， 它的⼯作原理是：
    - a. ⽣产者订单系统先发送⼀条half消息到Broker， half消息对消费者⽽⾔是不可⻅的
    - b. 再创建订单， 根据创建订单成功与否， 向Broker发送commit或rollback
    - c. 并且⽣产者订单系统还可以提供Broker回调接⼝， 当Broker发现⼀段时间half消息没有收到任 何操作命令， 则会主动调此接⼝来查询订单是否创建成功
    - d. ⼀旦half消息commit了， 消费者库存系统就会来消费， 如果消费成功， 则消息销毁， 分布式事 务成功结束
    - e. 如果消费失败， 则根据重试策略进⾏重试， 最后还失败则进⼊死信队列， 等待进⼀步处理
3. **Seata**：  阿⾥开源的分布式事务框架， ⽀持AT、TCC等多种模式， 底层都是基于两阶段提交理论来 实现的
:::

## 5 为什么 Dubbo 不用 JDK 的 SPI，而是要自己实现?
java spi缺点：
1、需要遍历所有实现并实例化，假设一个实现类初始化过程比较消耗资源且耗时，但是你的代码里面
又用不上它，这就产生了资源的浪费。也无法准确引用
2、没有使用缓存每次load都需要重新加载
Dubbo SPI： 
1、给每个实现类配了个名字，通过名字去文件里面找到对应的实现类全限定名然后加载实例化，按需加载。
2、`增加了缓存存储实例，提高读取性能`。
3、提供了对`IOC和AOP等高级功能的支持`，以实现更多类型的扩展

## 6 ZAB协议
什么是ZAB协议 ZAB协议是Zookeeper⽤来`实现⼀致性`的`原⼦⼴播协议`，该协议描述了Zookeeper是如何实现⼀致性 的，分为三个阶段： 

1. 领导者选举阶段：从Zookeeper集群中选出⼀个节点作为Leader，所有的写请求都会由Leader节点 来处理 

2. 数据同步阶段：集群中所有节点中的数据要和Leader节点保持⼀致，如果不⼀致则要进⾏同步 

3. 请求⼴播阶段：当Leader节点接收到写请求时，会利⽤两阶段提交来⼴播该写请求，使得写请求像事务⼀样在其他节点上执⾏，达到节点上的数据实时⼀致 但值得注意的是，Zookeeper只是尽量的在达到强⼀致性，实际上仍然只是最终⼀致性的。


## 7 什么是CAP理论

CAP理论是分布式领域中⾮常重要的⼀个指导理论， 
`C （Consistency） 表示强⼀致性`，
`A （Availability） 表示可⽤性`， 
`P （Partition Tolerance）  表示分区容错性`， 

CAP理论指出在⽬前的硬件条件下， ⼀个分布式系统是必须要保证分区容错性的， ⽽在这个前提下， 分布式系统要么保证CP， 要么保证AP， ⽆法同时保证CAP。

1. 分区容错性表示， ⼀个系统虽然是分布式的， 但是对外看上去应该是⼀个整体， 不能由于分布式系统内部的某个结点挂点， 或⽹络出现了故障， ⽽导致系统对外出现异常。所以， 对于分布式系统⽽⾔是⼀定
要保证分区容错性的。

2. 强⼀致性表示， ⼀个分布式系统中各个结点之间能及时的同步数据， 在数据同步过程中， 是不能对外提供服务的， 不然就会造成数据不⼀致， 所以强⼀致性和可⽤性是不能同时满⾜的。

3. 可⽤性表示， ⼀个分布式系统对外要保证可⽤ 。


## 8 什么是BASE理论

由于不能同时满⾜CAP， 所以出现了BASE理论：
1. BA：  Basically Available， 表示`基本可⽤， 表示可以允许⼀定程度的不可⽤`， ⽐如由于系统故障， 请求时间变⻓， 或者由于系统故障导致部分⾮核⼼功能不可⽤， 都是允许的
2. S：  Soft state：  表示`分布式系统可以处于⼀种中间状态`， ⽐如数据正在同步
3. E：  Eventually consistent， 表示`最终⼀致性`， 不要求分布式系统数据实时达到⼀致， 允许在经过⼀ 段时间后再达到⼀致， 在达到⼀致过程中， 系统也是可⽤的

## 9 什么是RPC

`RPC表示远程过程调⽤`， 对于Java这种⾯试对象语⾔， 也可以理解为远程⽅法调⽤

RPC调⽤和HTTP调⽤是有区别的， `RPC表示的是⼀种调⽤远程⽅法的⽅式， 可以使⽤HTTP协议 、或直接基于TCP 协议来实现RPC`。
在Java中， 我们可以通过直接使⽤某个服务接⼝的代理对象来执⾏⽅法， ⽽底层则通 过构造HTTP请求来调⽤远端的⽅法， 所以， 有⼀种说法是`RPC协议是HTTP协议之上的⼀种协议`， 也是可以理解的。


## 10 如何实现接口的幂等性
`唯一id`。每次操作，都根据操作和内容生成唯一的id，在执行之前先判断id是否存在，如果不存在
则执行后续操作，并且保存到数据库或者redis等。

`服务端提供发送token的接口`，业务调用接口前先获取token,然后调用业务接口请求时，把token带过去,务器判断token是否存在redis中，存在表示第一次请求，可以继续执行业务，执行业务完成后，最后需要把redis中的token删除

`建去重表`。将业务中有唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了

`版本控制`。增加版本号，当版本号符合时，才能更新数

`状态控制`。例如订单有状态`已支付 未支付 支付中 支付失败`，当处于未支付的时候才允许修改为支付中等


## 11 简述paxos算法
Paxos算法解决的是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，在一个分布
式数据库系统中，如果各个节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能够得
到一个一致的状态。为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算
法”以保证每个节点看到的指令一致。在Paxos算法中，有三种角色：Proposer (提议者),Acceptor（接
受者），Learners（记录员）
Proposer提议者：只要Proposer发的提案Propose被半数以上的Acceptor接受，Proposer就认为
该提案例的value被选定了。
Acceptor接受者：只要Acceptor接受了某个提案，Acceptor就认为该提案例的value被选定了
Learner记录员：Acceptor告诉Learner哪个value被选定，Learner就认为哪个value被选定。
Paxos算法分为两个阶段，具体如下：
阶段一 （preprae ）：
(a) Proposer 收到client请求或者发现本地有未提交的值，选择一个提案编号 N，然后向半数以上的
Acceptor 发送编号为 N 的 Prepare 请求。
(b) Acceptor 收到一个编号为 N 的 Prepare 请求，如果该轮paxos
本节点已经有已提交的value记录，对比记录的编号和N，大于N则拒绝回应，否则返回该记录
value及编号
没有已提交记录，判断本地是否有编号N1，N1>N、则拒绝响应，否则将N1改为N（如果没有
N1，则记录N），并响应prepare
阶段二 （accept）：
(a) 如果 Proposer 收到半数以上 Acceptor 对其发出的编号为 N 的 Prepare 请求的响应，那么它就会
发送一个针对[N,V]提案的 Accept 请求给半数以上的 Acceptor。V 就是收到的响应中编号最大的value
，如果响应中不包含任何value，那么V 就由 Proposer 自己决定。
(b) 如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，Acceptor对比本地的记录编号，如果
小于等于N，则接受该值，并提交记录value。否则拒绝请求
Proposer 如果收到的大多数Acceptor响应，则选定该value值，并同步给leaner，使未响应的Acceptor
达成一致


## 12 请谈谈ZooKeeper对事务性的支持
ZooKeeper对于事务性的支持主要依赖于四个函数，zoo_create_op_init， zoo_delete_op_init，
zoo_set_op_init以及zoo_check_op_init。
每一个函数都会在客户端初始化一个operation，客户端程序有义务保留这些operations。当准备好一
个事务中的所有操作后，可以使用zoo_multi来提交所有的操作，由zookeeper服务来保证这一系列操
作的原子性。也就是说只要其中有一个操作失败了，相当于此次提交的任何一个操作都没有对服务端的
数据造成影响。Zoo_multi的返回值是第一个失败操作的状态信号。


## 13 讲下Zookeeper watch机制

客户端，可以通过在znode上设置watch，实现实时监听znode的变化
`Watch事件是一个一次性的触发器`，当被设置了Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了Watch的客户端
父节点的创建，修改，删除都会触发Watcher事件。
子节点的创建，删除会触发Watcher事件。

一次性：一旦被触发就会移除，再次使用需要重新注册，因为每次变动都需要通知所有客户端，一次性可以减轻压力，3.6.0默认持久递归，可以触发多次
轻量：只通知发生了事件，不会告知事件内容，减轻服务器和带宽压力

`Watcher机制`包括三个角色：`客户端线程、客户端的 WatchManager 以及 ZooKeeper 服务器`
1. 客户端向 ZooKeeper 服务器注册一个 Watcher 监听，
2. 把这个监听信息存储到客户端的 WatchManager 中 
3. 当 ZooKeeper 中的节点发生变化时，会通知客户端，客户端会调用相应 Watcher 对象中的回调方法。watch回调是串行同步的



## 14 简述zk中的观察者机制

```sh
peerType=observer 
server.1:localhost:2181:3181:observer
```
观察者的设计是希望能动态扩展zookeeper集群又不会降低写性能。

如果扩展节点是follower，则写入操作提交时需要同步的节点数会变多，导致写入性能下降，而follower又是参与投票的、也会导致投票成本增加

observer是一种新的节点类型，解决扩展问题的同时，`不参与投票、只获取投票结果`，同时也可以处理

`读写请求，写请求转发给leader`。负责接收leader同步过来的提交数据，`observer的节点故障也不会影响集群的可用性`，

跨数据中心部署。把节点分散到多个数据中心可能因为网络的延迟会极大拖慢系统。使用observer的
话，更新操作都在一个单独的数据中心来处理，并发送到其他数据中心，让其他数据中心的节点消费数据。

无法完全消除数据中心之间的网络延迟，因为observer需要把更新请求转发到另一个数据中心的leader，并处理同步消息，网络速度极慢的话也会有影响，它的优势是为本地读请求提供快速响应。



## 15 Zookeeper 的典型应用场景
通过对 Zookeeper 中丰富的数据节点进行交叉使用，配合 Watcher 事件通知机制，可以非常方便的构
建一系列分布式应用中会涉及的核心功能，如：
（1）数据发布/订阅：配置中心
（2）负载均衡：提供服务者列表
（3）命名服务：提供服务名到服务地址的映射
（4）分布式协调/通知：watch机制和临时节点，获取各节点的任务进度，通过修改节点发出通知
（5）集群管理：是否有机器退出和加入、选举 master
（7）分布式锁
（8）分布式队列
第一类，在约定目录下创建临时目录节点，监听节点数目是否是要求的数目。
第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下
创建 PERSISTENT_SEQUENTIAL 节点，创建成功时Watcher 通知等待的队列，队列删除序列号最小的
节点用以消费。此场景下Zookeeper 的 znode 用于消息存储，znode 存储的数据就是消息队列中的消
息内容，SEQUENTIAL 序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必
担心队列消息的丢失问题。

## 16 简述TCC事务模型
TCC（补偿事务）：Try、Confirm、Cancel

针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作

Try操作做业务检查及资源预留，Confirm做业务确认操作，Cancel实现一个与Try相反的操作既回滚操
作。

TM首先发起所有的分支事务的try操作，任何一个分支事务的try操作执行失败，TM将会发起所有分支事务的Cancel操作，若try操作全部成功，TM将会发起所有分支事务的Confirm操作，其中Confirm/Cancel操作若执行失败，TM会进行重试

TCC模型对业务的侵入性较强，改造的难度较大，每个操作都需要有 try 、 confirm 、 cancel 三个接口实现

TCC 中会添加事务日志，如果 Confirm 或者 Cancel 阶段出错，则会进行重试，所以这两个阶段需要支持幂等；如果重试失败，则需要人工介入进行恢复和处理等。



# 17 负载均衡策略有哪些
1、轮询法
将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的
连接数和当前的系统负载。
2、加权轮询法
不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置
高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权
重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。
3、随机法
通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统
计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端
的每一台服务器，也就是轮询的结果。
4、加权随机法
与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是
按照权重随机请求后端服务器，而非顺序。
5、源地址哈希法
源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器
列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均
衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。
6、最小连接数法
最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根
据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，
尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。

## 18 分布式事务如何处理？怎么保证事务一致性？ 误区： 分布式事务 = Seata 分布式事务： 就是要将不同节点上的事务操作，提供操作原子性保证。同时成功或者同时失败。分布式事务第一个要点就是要在原本没有直接关联的事务之间建立联系。
`1、HTTP连接：` 最大努力通知。 -- 事后补偿。
`2、MQ` ： 事务消息机制。
`3、Redis：` 也可以定制出分布式事务机制。
`4、Seata` : 是通过TC来在多个事务之间建立联系。 两阶段：AT XA 就在于要锁资源。三阶段：TCC 在两阶段的基础上增加一个准备阶段。在准备阶段是不锁资源的。SAGA模式： 类似于熔断。 业务自己实现正向操作和补偿操作的逻辑。


`基于XA协议的`：两阶段提交和三阶段提交，需要数据库层面支持
`基于事务补偿机制的`：TCC，基于业务层面实现
`本地消息表`：基于本地数据库+mq，维护本地状态（进行中），通过mq调用服务，完成后响应一条消息回调，将状态改成完成。需要配合定时任务扫表、重新发送消息调用服务，需要保证幂等
`基于事务消息`：mq

## 19 session的分布式方案
1、采用无状态服务，抛弃session

2、`存入cookie（有安全风险）`
3、`服务器之间进行 Session 同步`，这样可以保证每个服务器上都有全部的 Session 信息，不过当服务
器数量比较多的时候，同步是会有延迟甚至同步失败；
4、 `IP 绑定策略`
使用 Nginx （或其他复杂均衡软硬件）中的 IP 绑定策略，同一个 IP 只能在指定的同一个机器访问，但
是这样做失去了负载均衡的意义，当挂掉一台服务器的时候，会影响一批用户的使用，风险很大；
5、`使用 Redis 存储`
把 Session 放到 Redis 中存储，虽然架构上变得复杂，并且需要多访问一次 Redis ，但是这种方案带来
的好处也是很大的：
```
1. 实现了 Session 共享；
2. 可以水平扩展（增加 Redis 服务器）；
3. 服务器重启 Session 不丢失（不过也要注意 Session 在 Redis 中的刷新/失效机制）；
4. 不仅可以跨服务器 Session 共享，甚至可以跨平台（例如网页端和 APP 端）。
```

## 20 分布式id生成方案
分布式ID是什么？ 有哪些解决⽅案？

在开发中， 我们通常会需要⼀个唯⼀ID来标识数据， 如果是单体架构， 我们可以通过数据库的主键， 或直接在内存中维护⼀个⾃增数字来作为ID都是可以的

但对于⼀个分布式系统， 就会有可能会出现ID冲突

:::tip 此时有以下解决⽅案：
1. uuid， 这种⽅案复杂度最低， 但是会影响存储空间和性能
2. 利⽤`单机数据库的⾃增主键`， 作为分布式ID的⽣成器， 复杂度适中， ID⻓度较之uuid更短， 但是受到单机数据库性能的限制， 并发量⼤的时候， 此⽅案也不是最优⽅案
3. 利⽤`redis、zookeeper的特性来⽣成id`， ⽐如`redis的⾃增命令`、`zookeeper的顺序节点`， 这种⽅案 和单机数据库(mysql)相⽐， 性能有所提⾼， 可以适当选⽤
4. `雪花算法`， ⼀切问题如果能直接⽤算法解决， 那就是最合适的， 利⽤雪花算法也可以⽣成分布式ID， 底层原理就是通过某台机器在某⼀毫秒内对某⼀个数字⾃增， 这种⽅案也能保证分布式架构中的系统id唯⼀， 但是只能保证趋势递增。业界存在tinyid、leaf等开源中间件实现了雪花算法。
:::

## 21 Dubbo⽀持哪些负载均衡策略 
1. 随机：从多个服务提供者随机选择⼀个来处理本次请求，调⽤量越⼤则分布越均匀，并⽀持按权重 设置随机概率 
2. 轮询：依次选择服务提供者来处理请求， 并⽀持按权重进⾏轮询，底层采⽤的是平滑加权轮询算法 
3. 最⼩活跃调⽤数：统计服务提供者当前正在处理的请求，下次请求过来则交给活跃数最⼩的服务器 来处理 
4. ⼀致性哈希：相同参数的请求总是发到同⼀个服务提供者



## 22 Zookeeper中的领导者选举的流程是怎样的？

`zxId：事务id，sId：节点id`
先对比zxId，再对比sId，先投自己，选票内容（zxId，sId），`遇强改投`
`投票箱`：每个节点在本地维护自己和其他节点的投票信息，`改投时需要更新信息，并广播`

节点状态：
LOOKING，竞选状态。
FOLLOWING，随从状态，同步leader状态，参与投票。
OBSERVING，观察状态,同步leader状态，不参与投票。
LEADING，领导者状态。


`RAFT算法`

对于Zookeeper集群，整个集群需要从集群节点中选出⼀个节点作为Leader，⼤体流程如下： 
1. 集群中各个节点⾸先都是观望状态（LOOKING），⼀开始都会投票给⾃⼰，认为⾃⼰⽐较适合作 为leader 
2. 然后相互交互投票，每个节点会收到其他节点发过来的选票，然后pk，先⽐较zxid，zxid⼤者获胜，zxid如果相等则⽐较myid，myid⼤者获胜 
3. ⼀个节点收到其他节点发过来的选票，经过PK后，如果PK输了，则改票，此节点就会投给zxid或 myid更⼤的节点，并将选票放⼊⾃⼰的投票箱中，并将新的选票发送给其他节点 
4. 如果pk是平局则将接收到的选票放⼊⾃⼰的投票箱中 
5. 如果pk赢了，则忽略所接收到的选票 
6. 当然⼀个节点将⼀张选票放⼊到⾃⼰的投票箱之后，就会从投票箱中统计票数，看是否超过⼀半的 节点都和⾃⼰所投的节点是⼀样的，如果超过半数，那么则认为当前⾃⼰所投的节点是leader 
7. 集群中每个节点都会经过同样的流程，pk的规则也是⼀样的，⼀旦改票就会告诉给其他服务器，所 以最终各个节点中的投票箱中的选票也将是⼀样的，所以各个节点最终选出来的leader也是⼀样 的，这样集群的leader就选举出来了

`崩溃选举`：
变更状态，leader故障后，follower进入`looking状态`
各节点投票，先投自己（zxId，sId），再广播投票，
接收到投票，对比zxId和sId，如果本节点小、则将票改为接收的投票信息，并记录投票信息，重新广播。否则本节点大、则可不做处理
统计本地投票信息，超过半数，则切换为leading状态并广播


## 23 Zookeeper集群中节点之间数据是如何同步的
1. ⾸先集群启动时，会先进⾏领导者选举，确定哪个节点是Leader，哪些节点是Follower和Observer 
2. 然后Leader会和其他节点进⾏数据同步，`采⽤发送快照和发送Diff⽇志的⽅式` --启动时，新加入节点时
3. 集群在⼯作过程中，所有的写请求都会交给Leader节点来进⾏处理，从节点只能处理读请求 
4. Leader节点收到⼀个写请求时，会通过`两阶段机制来处理` 
5. Leader节点会将该写请求对应的⽇志发送给其他Follower节点，并等待Follower节点持久化⽇志成功 
6. Follower节点收到⽇志后会进⾏持久化，如果持久化成功则发送⼀个`Ack给Leader节点 `
7. 当Leader节点收到半数以上的Ack后，就会开始提交，先更新Leader节点本地的内存数据 
8. 然后发送commit命令给Follower节点，Follower节点收到commit命令后就会更新各⾃本地内存数据 
9. 同时Leader节点还是将当前写请求直接发送给Observer节点，`Observer节点收到Leader发过来的 写请求后直接执⾏更新本地内存数据` 
10. 最后Leader节点返回客户端写请求响应成功 
11. 通过`同步机制和两阶段提交机制`来达到集群中节点数据⼀致



## 24 zk中一个客户端修改了某个节点的数据，其他客户端能够
马上获取到这个最新数据吗？
不能确保任何客户端能够获取（即Read Request）到一样的数据，除非客户端自己要求，方法是客户
`端在获取数据之前调用函数sync，再调用 getData()`
ZK客户端A对节点的内容从 v1->v2, 但是ZK客户端B对内容获取，依然得到的是 v1. 这个是实际存在的现象，存在同步时延


## 25 zk和eureka的区别

`zk`：`CP设计(强一致性)`，目标是一个分布式的协调系统，用于进行资源的统一管理。当节点crash后，需要进行leader的选举，在这个期间内，zk服务是不可用的。

`eureka`：`AP设计（高可用）`，目标是一个服务注册发现系统，专门用于微服务的服务发现注册。


Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和
查询服务。而Eureka的客户端在向某个Eureka注册时如果发现连接失败，会自动切换至其他节点，只
要有一台Eureka还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）

同时当eureka的服务端发现85%以上的服务都没有心跳的话，它就会认为自己的网络出了问题，就不会
从服务列表中删除这些失去心跳的服务，同时eureka的客户端也会缓存服务信息。eureka对于服务注
册发现来说是非常好的选择。


## 26 zk实际是如何存储生产者和消费者信息
服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址
服务消费者启动时: `订阅` /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向/dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址
监控中心启动时:` 订阅` /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。


## 27 zk分布式锁实现原理
上来直接创建一个锁节点下的一个接一个的临时顺序节点
如果自己不是第一个节点，就对自己上一个节点加监听器
只要上一个节点释放锁，自己就排到前面去了，相当于是一个排队机制。
而且用临时顺序节点，如果某个客户端创建临时顺序节点之后，自己宕机了，zk感知到那个客户端宕
机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消自己的排队。解决了惊群效应



## 28 Spring Cloud和Dubbo的区别
底层协议：`springcloud基于http协议`，`dubbo基于Tcp协议`，决定了dubbo的性能相对会比较好
注册中心：`Spring Cloud 使用的 eureka ，dubbo推荐使用zookeeper`
模型定义：dubbo 将一个接口定义为一个服务，SpringCloud 则是将一个应用定义为一个服务
SpringCloud是一个生态，而`Dubbo是SpringCloud生态中关于服务调用一种解决方案（服务治理）`


## 29 dubbo中Zk集群挂掉，发布者和订阅者还能通信么？
可以。
因为当启动dubbo容器时，消费者会去zookeeper拉取注册的生产者地址列表，并将其缓存在本地。每次发起调用时，都会按照本地的地址列表，以负载均衡的策略去进行调用。但是zookeeper挂掉则后续新的生产者无法被消费者发现。

注册中心对等集群，任意一台宕掉后，会自动切换到另一台
注册中心全部宕掉，服务提供者和消费者仍`可以通过本地缓存通讯`
服务提供者无状态，任一台宕机后，不影响使用
服务提供者全部宕机，服务消费者会无法使用，并无限次重连等待服务者恢复


## 6.5 分布式锁的使⽤场景是什么？ 有哪些实现⽅案？

> 在单体架构中， 多个线程都是属于同⼀个进程的， 所以在线程并发执⾏时， 遇到资源竞争时， 可以利⽤ `ReentrantLock、synchronized`等技术来作为锁， 来控制共享资源的使⽤ 。

⽽在分布式架构中， 多个线程是可能处于不同进程中的， ⽽这些线程并发执⾏遇到资源竞争时， 利⽤ReentrantLock、synchronized等技术是没办法来控制多个进程中的线程的， 所以需要分布式锁， 意思就是， 需要⼀个`分布式锁⽣成器`， 分布式系统中的应⽤程序都可以来使⽤这个⽣成器所提供的锁， 从⽽达到多个进程中的线程使⽤同⼀把锁。

:::tip ⽬前主流的分布式锁的实现⽅案有两种：
1. `zookeeper`：  **利⽤的是zookeeper的临时节点、顺序节点、watch机制来实现的**， zookeeper分布式 锁的特点是⾼⼀致性， 因为zookeeper保证的是CP， 所以由它实现的分布式锁更可靠， 不会出现混乱
2. `redis`：  **利⽤redis的setnx、lua脚本、消费订阅**等机制来实现的， redis分布式锁的特点是⾼可⽤ ， 因为redis保证的是AP， 所以由它实现的分布式锁可能不可靠， 不稳定 （⼀旦redis中的数据出现了 不⼀致） ， 可能会出现多个客户端同时加到锁的情况
:::



## 6.7 什么是ZAB协议

ZAB协议是Zookeeper⽤来`实现⼀致性的原⼦⼴播协议`， 该协议描述了Zookeeper是如何实现⼀致性的， 
:::tip 分为三个阶段：
1. 领导者选举阶段： 从Zookeeper集群中选出⼀个节点作为Leader， **所有的写请求都会由Leader节点来处理**
2. 数据同步阶段：  集群中所有节点中的数据要和Leader节点保持⼀致， 如果不⼀致则要进⾏同步
3. 请求⼴播阶段：  当Leader节点接收到写请求时， 会利⽤两阶段提交来⼴播该写请求， 使得写请求像事务⼀样在其他节点上执⾏， 达到节点上的数据实时⼀致
:::

但值得注意的是， **Zookeeper只是尽量的在达到强⼀致性， 实际上仍然只是最终⼀致性的**。


## 6.8 为什么Zookeeper可以⽤来作为注册中⼼

可以利⽤Zookeeper的临时节点和watch机制来实现注册中⼼的⾃动注册和发现， 另外Zookeeper中的  数据都是存在内存中的， 并且Zookeeper底层采⽤了nio， 多线程模型， 所以Zookeeper的性能也是⽐较 ⾼的， 所以可以⽤来作为注册中⼼， 但是如果考虑到注册中⼼应该是注册可⽤性的话， 那么Zookeeper  则不太合适， 因为Zookeeper是CP的， 它注重的是⼀致性， 所以集群数据不⼀致时， 集群将不可⽤， 所 以⽤Redis、Eureka、Nacos来作为注册中⼼将更合适。

## 6.9 Zookeeper中的领导者选举的流程是怎样的？
对于Zookeeper集群， 整个集群需要从集群节点中选出⼀个节点作为Leader， ⼤体流程如下：
1. 集群中各个节点⾸先都是观望状态 （LOOKING） ， ⼀开始都会投票给⾃⼰， 认为⾃⼰⽐较适合作 为leader
2. 然后相互交互投票， 每个节点会收到其他节点发过来的选票， 然后pk， 先⽐较zxid， zxid⼤者获 胜， zxid如果相等则⽐较myid， myid⼤者获胜
3. ⼀个节点收到其他节点发过来的选票， 经过PK后， 如果PK输了， 则改票， 此节点就会投给zxid或 myid更⼤的节点， 并将选票放⼊⾃⼰的投票箱中， 并将新的选票发送给其他节点
4. 如果pk是平局则将接收到的选票放⼊⾃⼰的投票箱中
5. 如果pk赢了， 则忽略所接收到的选票
6.  当然⼀个节点将⼀张选票放⼊到⾃⼰的投票箱之后， 就会从投票箱中统计票数， 看是否超过⼀半的 节点都和⾃⼰所投的节点是⼀样的， 如果超过半数， 那么则认为当前⾃⼰所投的节点是leader
7. 集群中每个节点都会经过同样的流程， pk的规则也是⼀样的， ⼀旦改票就会告诉给其他服务器， 所 以最终各个节点中的投票箱中的选票也将是⼀样的， 所以各个节点最终选出来的leader也是⼀样      的， 这样集群的leader就选举出来了




## 6.10 Zookeeper集群中节点之间数据是如何同步的
1. ⾸先集群启动时， 会先进⾏领导者选举， 确定哪个节点是Leader， 哪些节点是Follower和Observer
2. 然后Leader会和其他节点进⾏数据同步， 采⽤发送快照和发送Diff⽇志的⽅式
3. 集群在⼯作过程中， 所有的写请求都会交给Leader节点来进⾏处理， 从节点只能处理读请求
4. Leader节点收到⼀个写请求时， 会通过两阶段机制来处理
5. Leader节点会将该写请求对应的⽇志发送给其他Follower节点， 并等待Follower节点持久化⽇志成 功
6. Follower节点收到⽇志后会进⾏持久化， 如果持久化成功则发送⼀个Ack给Leader节点
7. 当Leader节点收到半数以上的Ack后， 就会开始提交， 先更新Leader节点本地的内存数据
8. 然后发送commit命令给Follower节点， Follower节点收到commit命令后就会更新各⾃本地内存数 据
9. 同时Leader节点还是将当前写请求直接发送给Observer节点， Observer节点收到Leader发过来的 写请求后直接执⾏更新本地内存数据
10. 最后Leader节点返回客户端写请求响应成功
11. 通过同步机制和两阶段提交机制来达到集群中节点数据⼀致

## 6.11 Dubbo⽀持哪些负载均衡策略
1. 随机： 从多个服务提供者随机选择⼀个来处理本次请求， 调⽤量越⼤则分布越均匀， 并⽀持按权重 设置随机概率
2. 轮询： 依次选择服务提供者来处理请求， 并⽀持按权重进⾏轮询， 底层采⽤的是平滑加权轮询算法
3. 最⼩活跃调⽤数： 统计服务提供者当前正在处理的请求， 下次请求过来则交给活跃数最⼩的服务器 来处理
4. ⼀致性哈希：  相同参数的请求总是发到同⼀个服务提供者


## 6.12 Dubbo是如何完成服务导出的？
1. ⾸先Dubbo会将程序员所使⽤的@DubboService注解或@Service注解进⾏解析得到程序员所定义 的服务参数， 包括定义的服务名、服务接⼝ 、服务超时时间 、服务协议等等， 得到⼀个ServiceBean。
2. 然后调⽤ServiceBean的export⽅法进⾏服务导出
3. 然后将服务信息注册到注册中⼼， 如果有多个协议， 多个注册中⼼， 那就将服务按单个协议， 单个 注册中⼼进⾏注册
4. 将服务信息注册到注册中⼼后， 还会绑定⼀些监听器， 监听动态配置中⼼的变更
5. 还会根据服务协议启动对应的Web服务器或⽹络框架， ⽐如Tomcat、Netty等


## 6.13 Dubbo是如何完成服务引⼊的？
1. 当程序员使⽤@Reference注解来引⼊⼀个服务时， Dubbo会将注解和服务的信息解析出来， 得到 当前所引⽤的服务名、服务接⼝是什么
2. 然后从注册中⼼进⾏查询服务信息， 得到服务的提供者信息， 并存在消费端的服务⽬录中
3. 并绑定⼀些监听器⽤来监听动态配置中⼼的变更
4. 然后根据查询得到的服务提供者信息⽣成⼀个服务接⼝的代理对象， 并放⼊Spring容器中作为Bean

## 6.14 Dubbo的架构设计是怎样的？

Dubbo中的架构设计是⾮常优秀的， 分为了很多层次， 并且每层都是可以扩展的， ⽐如：
1. Proxy服务代理层， ⽀持JDK动态代理、javassist等代理机制
2. Registry注册中⼼层， ⽀持Zookeeper、Redis等作为注册中⼼
3. Protocol远程调⽤层， ⽀持Dubbo、Http等调⽤协议
4. Transport⽹络传输层， ⽀持netty、mina等⽹络传输框架
5. Serialize数据序列化层， ⽀持JSON、Hessian等序列化机制


各层说明
●    config 配置层： 对外配置接⼝， 以为中⼼， 可以直接
初始化配置类， 也可以通过 spring 解析配置⽣成配置类
●    proxy 服务代理层： 服务接⼝透明代理， ⽣成服务的客户端 Stub 和服务器端 Skeleton, 以 
为中⼼， 扩展接⼝为  ProxyFactory
●    registry 注册中⼼层： 封装服务地址的注册与发现， 以服务 URL 为中⼼， 扩展接⼝为 
Registry , RegistryService
●    cluster 路由层： 封装多个提供者的路由及负载均衡， 并桥接注册中⼼， 以 Invoker 为中⼼， 扩 展接⼝为                                                                    LoadBalance
●    monitor 监控层： RPC 调⽤次数和调⽤时间监控， 以 为中⼼， 扩展接⼝为  Monitor , MonitorService
●    protocol 远程调⽤层： 封装 RPC 调⽤， 以 Result 为中⼼， 扩展接⼝为  Invoker , Exporter
●    exchange 信息交换层： 封装请求响应模式， 同步转异步， 以 Request , Response 为中⼼， 扩
展接⼝为

●    transport ⽹络传输层： 抽象 mina 和 netty 为统⼀接⼝， 以
Transporter , Client , Server , Codec
●    serialize 数据序列化层： 可复⽤的⼀些⼯具， 扩展接⼝为  Serialization , ObjectInput , 

关系说明
●    在 RPC 中， Protocol 是核⼼层， 也就是只要有 Protocol + Invoker + Exporter 就可以完成⾮透明 的 RPC 调⽤， 然后在 Invoker 的主过程上 Filter 拦截点。
●    图中的 Consumer 和 Provider 是抽象概念， 只是想让看图者更直观的了解哪些类分属于客户端与 服务器端， 不⽤ Client 和 Server 的原因是 Dubbo 在很多场景下都使⽤  Provider, Consumer,     Registry, Monitor 划分逻辑拓普节点， 保持统⼀概念。
●    ⽽ Cluster 是外围概念， 所以 Cluster 的⽬的是将多个 Invoker 伪装成⼀个 Invoker， 这样其它⼈ 只要关注 Protocol 层 Invoker 即可， 加上 Cluster 或者去掉 Cluster 对其它层都不会造成影响 ， 因为只有⼀个提供者时， 是不需要 Cluster 的。
●    Proxy 层封装了所有接⼝的透明化代理， ⽽在其它层都以 Invoker 为中⼼， 只有到了暴露给⽤户使 ⽤时， 才⽤  Proxy 将 Invoker 转成接⼝， 或将接⼝实现转成 Invoker， 也就是去掉 Proxy 层 RPC  是可以 Run 的， 只是不那么透明， 不那么看起来像调本地服务⼀样调远程服务。
●    ⽽ Remoting 实现是 Dubbo 协议的实现， 如果你选择 RMI 协议， 整个 Remoting 都不会⽤上，  Remoting 内部再划为 Transport 传输层和 Exchange 信息交换层， Transport 层只负责单向消息
传输， 是对 Mina, Netty, Grizzly 的抽象， 它也可以扩展 UDP 传输， ⽽ Exchange 层是在传输层 之上封装了 Request-Response 语义。
●    Registry 和 Monitor 实际上不算⼀层， ⽽是⼀个独⽴的节点， 只是为了全局概览， ⽤层的⽅式画在 ⼀起。

## 6.15 Spring Cloud有哪些常⽤组件， 作⽤是什么？
1. Eureka：  注册中⼼
2. Nacos： 注册中⼼ 、配置中⼼
3. Consul：  注册中⼼ 、配置中⼼
4. Spring Cloud Config：  配置中⼼
5. Feign/OpenFeign：  RPC调⽤
6. Kong：  服务⽹关
7. Zuul：  服务⽹关
8. Spring Cloud Gateway：  服务⽹关
9. Ribbon：  负载均衡
10. Spring CLoud Sleuth：  链路追踪
11. Zipkin：  链路追踪
12. Seata：  分布式事务
13. Dubbo： RPC调⽤
14. Sentinel：  服务熔断
15. Hystrix： 服务熔断


## 6.16 Spring Cloud和Dubbo有哪些区别？

Spring Cloud是⼀个微服务框架， 提供了微服务领域中的很多功能组件， Dubbo⼀开始是⼀个RPC调⽤框架， 核⼼是解决服务调⽤间的问题， Spring Cloud是⼀个⼤⽽全的框架， Dubbo则更侧重于服务调⽤， 所以Dubbo所提供的功能没有Spring Cloud全⾯， 但是Dubbo的服务调⽤性能⽐Spring Cloud⾼， 不过Spring Cloud和Dubbo并不是对⽴的， 是可以结合起来⼀起使⽤的。


## 6.17 什么是服务雪崩？ 什么是服务限流？
1.  当服务A调⽤服务B， 服务B调⽤C， 此时⼤量请求突然请求服务A， 假如服务A本身能抗住这些请求， 但是如果服务C抗不住， 导致服务C请求堆积， 从⽽服务B请求堆积， 从⽽服务A不可⽤， 这就是服务雪崩， 解决⽅式就是服务降级和服务熔断。
2. 服务限流是指在⾼并发请求下， 为了保护系统， 可以对访问服务的请求进⾏数量上的限制， 从⽽防 ⽌系统不被⼤量请求压垮， 在秒杀中， 限流是⾮常重要的。



## 6.18 什么是服务熔断？ 什么是服务降级？ 区别是什么？
1. 服务熔断是指， 当服务A调⽤的某个服务B不可⽤时， 上游服务A为了保证⾃⼰不受影响， 从⽽不再调⽤服务B， 直接返回⼀个结果， 减轻服务A和服务B的压⼒， 直到服务B恢复。
2. 服务降级是指， 当发现系统压⼒过载时， 可以通过关闭某个服务， 或限流某个服务来减轻系统压⼒， 这就是服务降级。

> 相同点：
1. 都是为了防⽌系统崩溃
2. 都让⽤户体验到某些功能暂时不可⽤

> 不同点： 熔断是下游服务故障触发的， 降级是为了降低系统负载

## 6.20 SOA、分布式、微服务之间有什么关系和区别？
1. 分布式架构是指将单体架构中的各个部分拆分， 然后部署不同的机器或进程中去， SOA和微服务基本上都是分布式架构的
2. SOA是⼀种⾯向服务的架构， 系统的所有服务都注册在总线上， 当调⽤服务时， 从总线上查找服务 信息， 然后调⽤
3. 微服务是⼀种更彻底的⾯向服务的架构， 将系统中各个功能个体抽成⼀个个⼩的应⽤程序， 基本保持⼀个应⽤对应的⼀个服务的架构


